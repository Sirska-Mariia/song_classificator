# -*- coding: utf-8 -*-
"""modeling_dl_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iv_4_kGjm51jd0r1jzMR3o4SH9aLZVo5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup

import pickle
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.utils.class_weight import compute_class_weight
import copy

"""Завантаження даних"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

DATA_PATH = Path('/content')
PROCESSED_FILE = DATA_PATH / 'processed_data_with_all_features.csv'

df = pd.read_csv(PROCESSED_FILE)
print(f"Розмір датасету: {df.shape}")
print(f"\nКолонки: {df.columns.tolist()}")

print(f"\nРозподіл по квадрантах:")
print(df['emotion_quadrant'].value_counts())

"""Підготовка даних"""

audio_features = [col for col in df.columns if any(x in col for x in
                  ['mfcc', 'chroma', 'spectral', 'tempo', 'zcr', 'rms',
                   'harmonic', 'percussive', 'mel', 'duration'])]

text_features = [col for col in df.columns if 'lyrics' in col]

print(f"\nАудіо features: {len(audio_features)}")
print(f"Текстові features: {len(text_features)}")

le = LabelEncoder()
df['emotion_label'] = le.fit_transform(df['emotion_quadrant'])

label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))

df_clean = df.dropna(subset=['emotion_quadrant', 'lyrics_processed'])

for col in audio_features:
    if col in df_clean.columns:
        df_clean[col].fillna(df_clean[col].median(), inplace=True)

"""TRAIN/VAL/TEST SPLIT"""

print(f"Колонки в df_clean: {df_clean.columns.tolist()[:10]}...")

train_file = DATA_PATH / 'tvt_dataframes' / 'tvt_70_15_15' / 'tvt_70_15_15_train_bimodal_balanced.csv'
val_file = DATA_PATH / 'tvt_dataframes' / 'tvt_70_15_15' / 'tvt_70_15_15_validate_bimodal_balanced.csv'
test_file = DATA_PATH / 'tvt_dataframes' / 'tvt_70_15_15' / 'tvt_70_15_15_test_bimodal_balanced.csv'

if train_file.exists():
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)
    test_df = pd.read_csv(test_file)

    print(f"Колонки в train_df: {train_df.columns.tolist()}")

    if 'Song' in train_df.columns:
        train_data = df_clean[df_clean['Audio_Song'].isin(train_df['Song'])].copy()
        val_data = df_clean[df_clean['Audio_Song'].isin(val_df['Song'])].copy()
        test_data = df_clean[df_clean['Audio_Song'].isin(test_df['Song'])].copy()
        print(f"Train matches: {len(train_data)}, Val matches: {len(val_data)}, Test matches: {len(test_data)}")

        if len(train_data) == 0:
            train_data = df_clean[df_clean['Lyric_Song'].isin(train_df['Song'])].copy()
            val_data = df_clean[df_clean['Lyric_Song'].isin(val_df['Song'])].copy()
            test_data = df_clean[df_clean['Lyric_Song'].isin(test_df['Song'])].copy()
            print(f"Train matches: {len(train_data)}, Val matches: {len(val_data)}, Test matches: {len(test_data)}")

        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
            train_data, temp_data = train_test_split(df_clean, test_size=0.3,
                                                     stratify=df_clean['emotion_label'],
                                                     random_state=42)
            val_data, test_data = train_test_split(temp_data, test_size=0.5,
                                                   stratify=temp_data['emotion_label'],
                                                   random_state=42)
    else:
        train_data, temp_data = train_test_split(df_clean, test_size=0.3,
                                                 stratify=df_clean['emotion_label'],
                                                 random_state=42)
        val_data, test_data = train_test_split(temp_data, test_size=0.5,
                                               stratify=temp_data['emotion_label'],
                                               random_state=42)
else:
    train_data, temp_data = train_test_split(df_clean, test_size=0.3,
                                             stratify=df_clean['emotion_label'],
                                             random_state=42)
    val_data, test_data = train_test_split(temp_data, test_size=0.5,
                                           stratify=temp_data['emotion_label'],
                                           random_state=42)

print(f"\nTrain size: {len(train_data)} ({len(train_data)/len(df_clean)*100:.1f}%)")
print(f"Val size: {len(val_data)} ({len(val_data)/len(df_clean)*100:.1f}%)")
print(f"Test size: {len(test_data)} ({len(test_data)/len(df_clean)*100:.1f}%)")

print("\nРозподіл класів:")
print("Train:", train_data['emotion_quadrant'].value_counts().to_dict())
print("Val:", val_data['emotion_quadrant'].value_counts().to_dict())
print("Test:", test_data['emotion_quadrant'].value_counts().to_dict())

class Config:
    MAX_LEN = 128
    TRAIN_BATCH_SIZE = 16
    VALID_BATCH_SIZE = 16
    EPOCHS = 10
    LEARNING_RATE = 1e-5

    DATA_PATH = Path('/content/sample_data')
    FILE_NAME = 'processed_data_with_all_features.csv'
    FULL_PATH = DATA_PATH / FILE_NAME

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

df = pd.read_csv(Config.FULL_PATH)

df.replace([np.inf, -np.inf], np.nan, inplace=True)

# 2. Визначаємо аудіо колонки
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
exclude_cols = ['label_encoded', 'cluster', 'kmeans_label', 'Unnamed: 0']
# Переконаємося, що не беремо target, якщо він числовий
if 'emotion_quadrant' in numeric_cols: exclude_cols.append('emotion_quadrant')

audio_feature_cols = [c for c in numeric_cols if c not in exclude_cols]

# 3. Перевіряємо NaN в аудіо даних
nan_count = df[audio_feature_cols].isna().sum().sum()
print(f"Знайдено NaN значень в аудіо даних: {nan_count}")

if nan_count > 0:
    print("Заповнюємо пропущені значення середнім...")
    df[audio_feature_cols] = df[audio_feature_cols].fillna(df[audio_feature_cols].mean())
    df[audio_feature_cols] = df[audio_feature_cols].fillna(0)

text_col = 'lyrics_raw'
if text_col not in df.columns:
     # Fallback
     text_col = [c for c in df.columns if 'lyric' in c.lower()][0]

print(f"Текстова колонка: {text_col}")
print(f"Кількість аудіо ознак: {len(audio_feature_cols)}")

target_col = 'emotion_quadrant'
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df[target_col])

scaler = StandardScaler()
df[audio_feature_cols] = scaler.fit_transform(df[audio_feature_cols])

if np.isnan(df[audio_feature_cols].values).any():
    df[audio_feature_cols] = df[audio_feature_cols].fillna(0)

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_encoded'])
train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

def mixup_data(x_audio, x_ids, x_mask, x_token, y, alpha=0.4):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x_audio.size(0)
    index = torch.randperm(batch_size).to(device)

    mixed_audio = lam * x_audio + (1 - lam) * x_audio[index, :]

    y_a, y_b = y, y[index]
    return mixed_audio, x_ids, x_mask, x_token, y_a, y_b, lam, index

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

class AudioMLP(nn.Module):
    def __init__(self, input_dim, output_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Dropout(0.5),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Dropout(0.5),

            nn.Linear(128, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.GELU()
        )
    def forward(self, x):
        return self.net(x)

class RobustModel(nn.Module):
    def __init__(self, num_classes, audio_input_dim):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')

        for param in self.bert.parameters():
            param.requires_grad = False
        for param in self.bert.encoder.layer[-1:].parameters():
            param.requires_grad = True

        self.audio_encoder = AudioMLP(audio_input_dim, output_dim=128)

        self.fusion_dim = 128
        self.text_proj = nn.Linear(768, self.fusion_dim)
        self.audio_proj = nn.Linear(128, self.fusion_dim)
        self.attention = nn.Linear(self.fusion_dim * 2, 1)

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.fusion_dim, num_classes)
        )

    def forward(self, ids, mask, token_type_ids, audio_features):
        bert_out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)
        text_emb = bert_out.pooler_output # [Batch, 768]
        text_proj = torch.tanh(self.text_proj(text_emb))

        audio_proj = torch.tanh(self.audio_encoder(audio_features))

        concat = torch.cat([text_proj, audio_proj], dim=1)
        weights = torch.sigmoid(self.attention(concat))

        fused = weights * text_proj + (1 - weights) * audio_proj

        return self.classifier(fused)

model = RobustModel(num_classes=len(le.classes_), audio_input_dim=len(audio_feature_cols))
model.to(device)

y_train_indices = train_df['label_encoded'].values
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_indices), y=y_train_indices)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

EPOCHS = 50
optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)
scheduler = OneCycleLR(optimizer, max_lr=5e-4, steps_per_epoch=len(training_loader), epochs=EPOCHS)
loss_fn = nn.CrossEntropyLoss(weight=class_weights)

history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
best_f1 = 0
best_model_wts = copy.deepcopy(model.state_dict())
patience = 12
counter = 0

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for data in training_loader:
        ids = data['ids'].to(device)
        mask = data['mask'].to(device)
        token_type_ids = data['token_type_ids'].to(device)
        audio_features = data['audio_features'].to(device)
        targets = data['targets'].to(device)

        optimizer.zero_grad()

        if np.random.random() > 0.5:
            mixed_audio, _, _, _, y_a, y_b, lam, idx = mixup_data(audio_features, ids, mask, token_type_ids, targets)

            outputs = model(ids, mask, token_type_ids, mixed_audio)

            loss = mixup_criterion(loss_fn, outputs, y_a, y_b, lam)
        else:
            outputs = model(ids, mask, token_type_ids, audio_features)
            loss = loss_fn(outputs, targets)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()

    model.eval()
    val_loss = 0
    fin_targets = []
    fin_outputs = []

    with torch.no_grad():
        for data in testing_loader:
            ids = data['ids'].to(device)
            mask = data['mask'].to(device)
            token_type_ids = data['token_type_ids'].to(device)
            audio_features = data['audio_features'].to(device)
            targets = data['targets'].to(device)

            outputs = model(ids, mask, token_type_ids, audio_features)
            loss = loss_fn(outputs, targets)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            fin_targets.extend(targets.cpu().tolist())
            fin_outputs.extend(predicted.cpu().tolist())

    avg_train_loss = total_loss / len(training_loader)
    avg_val_loss = val_loss / len(testing_loader)
    val_f1 = f1_score(fin_targets, fin_outputs, average='weighted')
    val_acc = accuracy_score(fin_targets, fin_outputs)

    history['train_loss'].append(avg_train_loss)
    history['val_loss'].append(avg_val_loss)
    history['val_f1'].append(val_f1)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | F1: {val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1
        best_model_wts = copy.deepcopy(model.state_dict())
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early Stopping.")
            break

print(f"\nBest Weighted F1: {best_f1:.4f}")
model.load_state_dict(best_model_wts)

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train')
plt.plot(history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history['val_f1'], label='Val F1', color='green')
plt.title('F1 Score')
plt.legend()
plt.show()

model.eval()
fin_targets = []
fin_outputs = []
with torch.no_grad():
    for data in testing_loader:
        outputs = model(data['ids'].to(device), data['mask'].to(device),
                       data['token_type_ids'].to(device), data['audio_features'].to(device))
        _, predicted = torch.max(outputs, 1)
        fin_targets.extend(data['targets'].tolist())
        fin_outputs.extend(predicted.tolist())

print(classification_report(fin_targets, fin_outputs, target_names=le.classes_))

"""**Combined model:**"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 16
MAX_LEN = 128
EPOCHS = 40
LEARNING_RATE = 5e-5
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class MultimodalDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len, text_col, audio_cols):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.text = dataframe[text_col].astype(str)
        self.targets = dataframe['label_encoded'].values
        self.audio = dataframe[audio_cols].values
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        text = " ".join(text.split())

        inputs = self.tokenizer.encode_plus(
            text, None, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', return_token_type_ids=True, truncation=True
        )

        return {
            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),
            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),
            'audio_features': torch.tensor(self.audio[index], dtype=torch.float),
            'targets': torch.tensor(self.targets[index], dtype=torch.long)
        }

def mixup_data(x_audio, x_ids, x_mask, x_token, y, alpha=0.4):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x_audio.size(0)
    index = torch.randperm(batch_size).to(device)

    mixed_audio = lam * x_audio + (1 - lam) * x_audio[index, :]

    y_a, y_b = y, y[index]
    return mixed_audio, x_ids, x_mask, x_token, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

training_set = MultimodalDataset(train_df, tokenizer, MAX_LEN, text_col, audio_feature_cols)
testing_set = MultimodalDataset(test_df, tokenizer, MAX_LEN, text_col, audio_feature_cols)

training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)
testing_loader = DataLoader(testing_set, batch_size=BATCH_SIZE, shuffle=False)

class AudioMLP(nn.Module):
    def __init__(self, input_dim, output_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.GELU(),
            nn.Dropout(0.4),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Dropout(0.4),

            nn.Linear(128, output_dim), # [Batch, 128]
            nn.BatchNorm1d(output_dim),
            nn.GELU()
        )
    def forward(self, x):
        return self.net(x)

class CombinedModel(nn.Module):
    def __init__(self, num_classes, audio_input_dim):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        for param in self.bert.parameters():
            param.requires_grad = False
        for param in self.bert.encoder.layer[-2:].parameters():
            param.requires_grad = True
        self.audio_encoder = AudioMLP(audio_input_dim, output_dim=128)

        self.fusion_layer = nn.Linear(768 + 128, 256)
        self.dropout = nn.Dropout(0.5)
        self.classifier = nn.Linear(256, num_classes)

    def forward(self, ids, mask, token_type_ids, audio_features):
        bert_out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)
        text_emb = bert_out.pooler_output

        audio_emb = self.audio_encoder(audio_features)

        combined = torch.cat((text_emb, audio_emb), dim=1)

        x = torch.relu(self.fusion_layer(combined))
        x = self.dropout(x)
        output = self.classifier(x)
        return output

model = CombinedModel(num_classes=len(le.classes_), audio_input_dim=len(audio_feature_cols))
model.to(device)

y_train_indices = train_df['label_encoded'].values
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_indices), y=y_train_indices)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(training_loader), epochs=EPOCHS)
loss_fn = nn.CrossEntropyLoss(weight=class_weights)

history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
best_f1 = 0
best_model_wts = copy.deepcopy(model.state_dict())
patience = 10
counter = 0

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for data in training_loader:
        ids = data['ids'].to(device)
        mask = data['mask'].to(device)
        token_type_ids = data['token_type_ids'].to(device)
        audio_features = data['audio_features'].to(device)
        targets = data['targets'].to(device)

        optimizer.zero_grad()

        if np.random.random() > 0.5:
            mixed_audio, _, _, _, y_a, y_b, lam = mixup_data(audio_features, ids, mask, token_type_ids, targets)
            outputs = model(ids, mask, token_type_ids, mixed_audio)
            loss = mixup_criterion(loss_fn, outputs, y_a, y_b, lam)
        else:
            outputs = model(ids, mask, token_type_ids, audio_features)
            loss = loss_fn(outputs, targets)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()

    model.eval()
    val_loss = 0
    fin_targets = []
    fin_outputs = []

    with torch.no_grad():
        for data in testing_loader:
            ids = data['ids'].to(device)
            mask = data['mask'].to(device)
            token_type_ids = data['token_type_ids'].to(device)
            audio_features = data['audio_features'].to(device)
            targets = data['targets'].to(device)

            outputs = model(ids, mask, token_type_ids, audio_features)
            loss = loss_fn(outputs, targets)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            fin_targets.extend(targets.cpu().tolist())
            fin_outputs.extend(predicted.cpu().tolist())

    avg_train_loss = total_loss / len(training_loader)
    avg_val_loss = val_loss / len(testing_loader)
    val_acc = accuracy_score(fin_targets, fin_outputs)
    val_f1 = f1_score(fin_targets, fin_outputs, average='weighted')

    history['train_loss'].append(avg_train_loss)
    history['val_loss'].append(avg_val_loss)
    history['val_acc'].append(val_acc)
    history['val_f1'].append(val_f1)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1
        best_model_wts = copy.deepcopy(model.state_dict())
        counter = 0
        torch.save(model.state_dict(), 'best_combined_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print("Early Stopping!")
            break

model.load_state_dict(best_model_wts)
print(f"\nНайкращий Weighted F1: {best_f1:.4f}")

plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
plt.plot(history['train_loss'], label='Train Loss', marker='.')
plt.plot(history['val_loss'], label='Val Loss', marker='.')
plt.title('Loss Evolution')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(history['val_acc'], label='Val Accuracy', color='green', marker='.')
plt.title('Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(history['val_f1'], label='Val F1 Score', color='red', marker='.')
plt.title('Validation F1 Score')
plt.xlabel('Epochs')
plt.ylabel('F1')
plt.grid(True)

plt.show()

model.eval()
fin_targets = []
fin_outputs = []
with torch.no_grad():
    for data in testing_loader:
        ids = data['ids'].to(device)
        mask = data['mask'].to(device)
        token_type_ids = data['token_type_ids'].to(device)
        audio_features = data['audio_features'].to(device)
        targets = data['targets'].to(device)

        outputs = model(ids, mask, token_type_ids, audio_features)
        _, predicted = torch.max(outputs, 1)

        fin_targets.extend(targets.cpu().tolist())
        fin_outputs.extend(predicted.cpu().tolist())

print(classification_report(fin_targets, fin_outputs, target_names=le.classes_))

from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

FULL_PATH = DATA_PATH / 'processed_data_with_all_features.csv'

BATCH_SIZE = 16
MAX_LEN = 128
EPOCHS = 15
LEARNING_RATE = 2e-5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

df = pd.read_csv(FULL_PATH)

text_col = 'lyrics_raw'
target_col = 'emotion_quadrant'
if text_col not in df.columns: text_col = 'lyrics'

le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df[target_col])

group_col = 'Audio_Song'
if group_col not in df.columns: group_col = 'filename'

splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idxs, test_idxs = next(splitter.split(df, groups=df[group_col]))

train_df = df.iloc[train_idxs].reset_index(drop=True)
test_df = df.iloc[test_idxs].reset_index(drop=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len, text_col):
        self.tokenizer = tokenizer
        self.text = dataframe[text_col].astype(str)
        self.targets = dataframe['label_encoded'].values
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        text = " ".join(text.split())

        inputs = self.tokenizer.encode_plus(
            text, None, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', return_token_type_ids=True, truncation=True
        )

        return {
            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),
            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.long)
        }

training_set = TextDataset(train_df, tokenizer, MAX_LEN, text_col)
testing_set = TextDataset(test_df, tokenizer, MAX_LEN, text_col)

training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)
testing_loader = DataLoader(testing_set, batch_size=BATCH_SIZE, shuffle=False)

class TextBERT(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')

        for param in self.bert.parameters():
            param.requires_grad = False
        for param in self.bert.encoder.layer[-2:].parameters():
            param.requires_grad = True

        self.drop = nn.Dropout(0.3)
        self.classifier = nn.Linear(768, num_classes)

    def forward(self, ids, mask, token_type_ids):
        output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)
        x = self.drop(output.pooler_output)
        return self.classifier(x)

model = TextBERT(num_classes=len(le.classes_))
model.to(device)

y_train = train_df['label_encoded'].values
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE, steps_per_epoch=len(training_loader), epochs=EPOCHS)
loss_fn = nn.CrossEntropyLoss(weight=class_weights)

history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}
best_f1 = 0
best_model_wts = copy.deepcopy(model.state_dict())


for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for data in training_loader:
        ids = data['ids'].to(device)
        mask = data['mask'].to(device)
        token_type_ids = data['token_type_ids'].to(device)
        targets = data['targets'].to(device)

        optimizer.zero_grad()
        outputs = model(ids, mask, token_type_ids)
        loss = loss_fn(outputs, targets)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()

    model.eval()
    val_loss = 0
    fin_targets = []
    fin_outputs = []
    with torch.no_grad():
        for data in testing_loader:
            ids = data['ids'].to(device)
            mask = data['mask'].to(device)
            token_type_ids = data['token_type_ids'].to(device)
            targets = data['targets'].to(device)

            outputs = model(ids, mask, token_type_ids)
            loss = loss_fn(outputs, targets)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            fin_targets.extend(targets.cpu().tolist())
            fin_outputs.extend(predicted.cpu().tolist())

    avg_train_loss = total_loss / len(training_loader)
    avg_val_loss = val_loss / len(testing_loader)
    val_f1 = f1_score(fin_targets, fin_outputs, average='weighted')
    val_acc = accuracy_score(fin_targets, fin_outputs)

    history['train_loss'].append(avg_train_loss)
    history['val_loss'].append(avg_val_loss)
    history['val_f1'].append(val_f1)
    history['val_acc'].append(val_acc)

    print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | F1: {val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1
        best_model_wts = copy.deepcopy(model.state_dict())

print(f"\nText-Only Best F1: {best_f1:.4f}")

model.load_state_dict(best_model_wts)

plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(history['train_loss'], label='Train')
plt.plot(history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(history['val_acc'], color='green', label='Accuracy')
plt.title('Accuracy')

plt.subplot(1, 3, 3)
plt.plot(history['val_f1'], color='red', label='F1 Score')
plt.title('F1 Score')
plt.show()

model.eval()
fin_targets = []
fin_outputs = []
with torch.no_grad():
    for data in testing_loader:
        outputs = model(data['ids'].to(device), data['mask'].to(device), data['token_type_ids'].to(device))
        _, predicted = torch.max(outputs, 1)
        fin_targets.extend(data['targets'].tolist())
        fin_outputs.extend(predicted.tolist())

print(classification_report(fin_targets, fin_outputs, target_names=le.classes_))
